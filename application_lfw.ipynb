{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Python 3.12.8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from collections import Counter\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset\n",
    "> Labelled Faces in the Wild Dataset\n",
    "\n",
    "    people.csv:\n",
    "> Contains randomly generated splits for 10-fold cross validation specifically for individual faces. \n",
    "\n",
    "> There are 10 total sets, each with a different amount of people:\n",
    "> Set 1: 601. Set 2: 555. Set 3: 552. Set 4: 560. Set 5: 567. Set 6: 527. Set 7: 597. Set 8: 601. Set 9: 580. Set 10: 609."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LFW dataset with a minimum of 70 images per person\n",
    "# and resize the images to 40% of their original size\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes for plotting later on\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "y = lfw_people.target # Target values are the person's ID labels\n",
    "target_names = lfw_people.target_names # Person's name\n",
    "\n",
    "print(\"Number of samples: %d\" % n_samples) # Number of samples = rows in X\n",
    "print(\"Number of features: %d\" % n_features) # Number of features = columns in X\n",
    "print(\"Number of classes: %d\" % len(target_names)) # Number of classes = number of unique values in y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "def apply_pca(X_train, X_test):\n",
    "    n_components = 150\n",
    "    print(\n",
    "    \"Extracting the top %d eigenfaces from %d faces\" % (n_components, X_train.shape[0])\n",
    "    )\n",
    "    pca = PCA(n_components=n_components, svd_solver=\"randomized\", whiten=True).fit(X_train)\n",
    "    eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "    print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    return X_train_pca, X_test_pca, eigenfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best SVM Model with GridSearchCV\n",
    "def findTheBestSVMModel(X_train, y_train):\n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid_svm = {\n",
    "        'C': [0.1, 1],                   \n",
    "        'gamma': [0.001, 0.01, 0.1],                \n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'class_weight': ['balanced', None]                    \n",
    "    }\n",
    "\n",
    "    # Use StratifiedKFold for cross-validation\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "    svmModel_grid = GridSearchCV(\n",
    "        estimator=SVC(probability=True, max_iter=5000),\n",
    "        param_grid=param_grid_svm, \n",
    "        verbose=2,\n",
    "        cv=cv, \n",
    "        n_jobs=4) # n_jobs=cores used, -1 = all cores\n",
    "    \n",
    "    svmModel_grid.fit(X_train, y_train)\n",
    "    best_model = svmModel_grid.best_estimator_\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate the models\n",
    "def predict_and_evaluate(model, X_test, y_test, target_names):\n",
    "    y_pred = model.predict(X_test) # Predict the labels\n",
    "    unique_labels = np.unique(y_test) # Get the unique labels\n",
    "    print(classification_report(y_test, y_pred, labels=unique_labels, target_names=target_names, zero_division=0))\n",
    "    print(confusion_matrix(y_test, y_pred, labels=unique_labels))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset\n",
    "    train_test_split()\n",
    "\n",
    "> Split **the dataset** into training and testing sets (20% for testing and 80% for training); \n",
    "> Choose random_state as a fixed seed value to ensure that the random processes (spliting the data) produce the same results every run; \n",
    "> Stratify the split to ensure that the same proportion of classes are present in both sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (20% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Number of training samples: %d\" % X_train.shape[0])\n",
    "print(\"Number of testing samples: %d\" % X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Method - Support Vector Machine (SVM)\n",
    "\n",
    "### Scalling the data\n",
    "    StandardScaler()\n",
    "\n",
    "> Standardize the features by removing the mean and scaling to unit variance before splitting the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features by removing the mean and scaling to unit variance before splitting the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Number of training samples: %d\" % X_train.shape[0])\n",
    "print(\"Number of testing samples: %d\" % X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "X_train_pca, X_test_pca, eigenfaces = apply_pca(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the SVM Model for original and PCA-reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM on Original Data\n",
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Ariel Sharon       0.67      0.75      0.71        16\n",
      "     Colin Powell       0.86      0.94      0.90        47\n",
      "  Donald Rumsfeld       0.72      0.75      0.73        24\n",
      "    George W Bush       0.93      0.92      0.92       106\n",
      "Gerhard Schroeder       0.72      0.82      0.77        22\n",
      "      Hugo Chavez       0.83      0.71      0.77        14\n",
      "       Tony Blair       0.91      0.72      0.81        29\n",
      "\n",
      "         accuracy                           0.85       258\n",
      "        macro avg       0.81      0.80      0.80       258\n",
      "     weighted avg       0.86      0.85      0.85       258\n",
      "\n",
      "[[12  0  1  1  1  1  0]\n",
      " [ 3 44  0  0  0  0  0]\n",
      " [ 1  1 18  1  1  0  2]\n",
      " [ 1  3  4 97  1  0  0]\n",
      " [ 0  0  1  2 18  1  0]\n",
      " [ 1  1  0  1  1 10  0]\n",
      " [ 0  2  1  2  3  0 21]]\n",
      "Accuracy: 0.85\n",
      "SVM on PCA Data\n",
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Ariel Sharon       0.74      0.88      0.80        16\n",
      "     Colin Powell       0.86      0.91      0.89        47\n",
      "  Donald Rumsfeld       0.82      0.75      0.78        24\n",
      "    George W Bush       0.93      0.91      0.92       106\n",
      "Gerhard Schroeder       0.64      0.82      0.72        22\n",
      "      Hugo Chavez       0.92      0.79      0.85        14\n",
      "       Tony Blair       0.88      0.72      0.79        29\n",
      "\n",
      "         accuracy                           0.86       258\n",
      "        macro avg       0.83      0.82      0.82       258\n",
      "     weighted avg       0.86      0.86      0.86       258\n",
      "\n",
      "[[14  1  1  0  0  0  0]\n",
      " [ 1 43  1  2  0  0  0]\n",
      " [ 1  0 18  3  1  0  1]\n",
      " [ 1  3  2 96  2  1  1]\n",
      " [ 1  0  0  2 18  0  1]\n",
      " [ 1  1  0  0  1 11  0]\n",
      " [ 0  2  0  0  6  0 21]]\n",
      "Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate SVM on original data\n",
    "print(\"SVM on Original Data\")\n",
    "svm = findTheBestSVMModel(X_train, y_train)\n",
    "y_pred_original = predict_and_evaluate(svm, X_test, y_test, target_names)\n",
    "\n",
    "# Train and evaluate SVM on PCA data\n",
    "print(\"SVM on PCA Data\")\n",
    "svm_pca = findTheBestSVMModel(X_train_pca, y_train)\n",
    "y_pred_pca = predict_and_evaluate(svm_pca, X_test_pca, y_test, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised Learning Method - SelfTrainingClassifier with an SVM base estimator\n",
    "\n",
    "### Splitting and Scalling for the Semi-Supervised SVM (SelfTrainingClassifier)\n",
    "1. First test_train_split: \n",
    "\n",
    "> Split **the training set** into labeled (30%) and unlabeled (70%) sets \n",
    "\n",
    "2. Combine the training data\n",
    "\n",
    "> Combine labeled and unlabeled **training data** for scalling and PCA.\n",
    "> This step is necessary because semi-supervised learning requires access to both labeled and unlabeled data\n",
    "\n",
    "3. StandardScaler():\n",
    "\n",
    "> Standardize the features by removing the mean and scaling to unit variance before splitting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into labeled (30%) and unlabeled (70%) sets\n",
    "X_train_labeled, X_train_unlabeled, y_train_labeled, y_train_unlabeled = train_test_split(\n",
    "    X_train, y_train, test_size=0.7, random_state=42)\n",
    "\n",
    "# Combine labeled and unlabeled training data\n",
    "X_train_combined = np.vstack((X_train_labeled, X_train_unlabeled))\n",
    "y_train_combined = np.concatenate((y_train_labeled, [-1] * len(y_train_unlabeled))) # -1 for unlabeled samples\n",
    "\n",
    "# Scale the combined training data\n",
    "scaler = StandardScaler()\n",
    "X_train_combined_scaled = scaler.fit_transform(X_train_combined)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Number of combined training samples: %d\" % X_train_combined_scaled.shape[0])\n",
    "print(\"Number of testing samples: %d\" % X_test_scaled.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "X_train_combined_pca, X_test_pca, eigenfaces = apply_pca(X_train_combined_scaled, X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the semi-supervised model using SelfTrainingClassifier with an SVM base estimator\n",
    "\n",
    "1. Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n",
      "SVC(C=0.1, class_weight='balanced', gamma=0.001, kernel='linear', max_iter=5000,\n",
      "    probability=True)\n",
      "End of iteration 1, added 49 new labels.\n",
      "End of iteration 2, added 3 new labels.\n",
      "End of iteration 3, added 1 new labels.\n",
      "End of iteration 4, added 6 new labels.\n",
      "Self-Training Model on Original Data\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Ariel Sharon       0.60      0.56      0.58        16\n",
      "     Colin Powell       0.86      0.89      0.88        47\n",
      "  Donald Rumsfeld       0.79      0.62      0.70        24\n",
      "    George W Bush       0.82      0.92      0.87       106\n",
      "Gerhard Schroeder       0.59      0.59      0.59        22\n",
      "      Hugo Chavez       1.00      0.50      0.67        14\n",
      "       Tony Blair       0.68      0.66      0.67        29\n",
      "\n",
      "         accuracy                           0.78       258\n",
      "        macro avg       0.76      0.68      0.71       258\n",
      "     weighted avg       0.79      0.78      0.78       258\n",
      "\n",
      "[[ 9  1  1  3  1  0  1]\n",
      " [ 3 42  0  2  0  0  0]\n",
      " [ 0  0 15  7  0  0  2]\n",
      " [ 2  2  1 97  4  0  0]\n",
      " [ 0  0  1  3 13  0  5]\n",
      " [ 0  2  0  4  0  7  1]\n",
      " [ 1  2  1  2  4  0 19]]\n",
      "Accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "# Train the semi-supervised model using SelfTrainingClassifier with an SVM base estimator\n",
    "# On the original combined training data\n",
    "base_estimator = findTheBestSVMModel(X_train_labeled, y_train_labeled)\n",
    "\n",
    "# Print base_estimator to confirm the best parameters selected by findTheBestSVMModel\n",
    "print(base_estimator)\n",
    "\n",
    "# Train the SelfTrainingClassifier model on the original combined training data\n",
    "self_training_model_original = SelfTrainingClassifier(\n",
    "    estimator=base_estimator, \n",
    "    criterion='threshold', \n",
    "    threshold=0.95, \n",
    "    # k_best=10, \n",
    "    max_iter=20, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "self_training_model_original_fit = self_training_model_original.fit(X_train_combined_scaled, y_train_combined)\n",
    "\n",
    "# Passing the model to the predict_and_evaluate function for predicting the labels and evaluating the model\n",
    "print(\"Self-Training Model on Original Data\")\n",
    "y_pred_self_training_original_final = predict_and_evaluate(\n",
    "    self_training_model_original_fit, \n",
    "    X_test_scaled, \n",
    "    y_test, \n",
    "    target_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. PCA-reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the semi-supervised model\n",
    "# On the PCA-reduced combined training data\n",
    "self_training_model_pca = SelfTrainingClassifier(base_estimator, criterion='threshold', threshold=0.95, max_iter=20, verbose=True)\n",
    "self_training_model_pca.fit(X_train_combined_pca, y_train_combined)\n",
    "y_pred_self_training_pca = self_training_model_pca.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Training Model on PCA Data\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Ariel Sharon       0.67      0.38      0.48        16\n",
      "     Colin Powell       0.77      0.79      0.78        47\n",
      "  Donald Rumsfeld       0.78      0.58      0.67        24\n",
      "    George W Bush       0.73      0.89      0.80       106\n",
      "Gerhard Schroeder       0.54      0.59      0.57        22\n",
      "      Hugo Chavez       1.00      0.50      0.67        14\n",
      "       Tony Blair       0.78      0.62      0.69        29\n",
      "\n",
      "         accuracy                           0.73       258\n",
      "        macro avg       0.75      0.62      0.66       258\n",
      "     weighted avg       0.74      0.73      0.72       258\n",
      "\n",
      "[[ 6  1  1  8  0  0  0]\n",
      " [ 1 37  0  6  3  0  0]\n",
      " [ 0  3 14  6  0  0  1]\n",
      " [ 2  5  0 94  3  0  2]\n",
      " [ 0  1  2  4 13  0  2]\n",
      " [ 0  0  0  5  2  7  0]\n",
      " [ 0  1  1  6  3  0 18]]\n",
      "Accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "print(\"Self-Training Model on PCA Data\")\n",
    "y_pred_self_training_pca_final = predict_and_evaluate(self_training_model_pca, X_test_pca, y_test, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some of the test results\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return f'predicted: {pred_name}\\ntrue:      {true_name}'\n",
    "\n",
    "prediction_titles_original = [title(y_pred_original, y_test, target_names, i) for i in range(y_pred_original.shape[0])]\n",
    "prediction_titles_pca = [title(y_pred_pca, y_test, target_names, i) for i in range(y_pred_pca.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles_original, h, w)\n",
    "plt.show()\n",
    "\n",
    "plot_gallery(X_test, prediction_titles_pca, h, w)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
